import os
import time
import json
import requests
from concurrent.futures import ThreadPoolExecutor

# å›¾ç‰‡å¤„ç†åº“ (ç”¨äºåˆæˆ PDF)
from PIL import Image

# è‡ªåŠ¨åŒ–æµè§ˆå™¨æ¨¡å—
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

# ================= é…ç½®åŒºåŸŸ =================
MAX_THREADS = 16
# ===========================================

def get_headers():
    return {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
    }

def fetch_book_data(url):
    print(f"ğŸ•µï¸â€â™‚ï¸ æ­£åœ¨å¯åŠ¨éšå½¢æµè§ˆå™¨ (åˆ†æé¡µé¢)...")
    options = Options()
    
    # ã€ä¿®æ”¹ç‚¹1ã€‘å¼€å¯æ— å¤´æ¨¡å¼ (éšèº«)
    options.add_argument("--headless") 
    
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1920,1080")
    options.add_experimental_option('excludeSwitches', ['enable-logging'])

    driver = None
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        driver.get(url)
        
        print("â³ ç­‰å¾…åŠ å¯†æ¨¡å—è§£å¯† (10 ç§’)...")
        time.sleep(2) 
        
        # ä¸‡èƒ½æå–è„šæœ¬
        extract_script = """
        try {
            var candidates = [
                window.fliphtml5_pages, window.configForPages,
                (window.bookConfig && window.bookConfig.pages),
                (window.htmlConfig && window.htmlConfig.pages),
                (window.sliderConfig && window.sliderConfig.pages)
            ];
            var pages = [];
            for(var i=0; i<candidates.length; i++) {
                if(candidates[i] && Array.isArray(candidates[i]) && candidates[i].length > 0) {
                    pages = candidates[i]; break;
                }
            }
            if(pages.length === 0 && window.bookConfig) {
                 for(var key in window.bookConfig) {
                    if(Array.isArray(window.bookConfig[key]) && window.bookConfig[key].length > 0) {
                        var first = window.bookConfig[key][0];
                        if(first && (first.path || first.url || first.image)) {
                            pages = window.bookConfig[key]; break;
                        }
                    }
                }
            }
            var result = [];
            for(var i=0; i<pages.length; i++) {
                var p = pages[i];
                if(typeof p === 'string') result.push(p);
                else if(p.path) result.push(p.path);
                else if(p.url) result.push(p.url);
                else if(p.image) result.push(p.image);
                else if(p.n && p.n[0]) result.push(p.n[0]);
            }
            return { title: document.title, pages: result };
        } catch(e) { return null; }
        """
        return driver.execute_script(extract_script)
    except Exception as e:
        print(f"âŒ æµè§ˆå™¨é”™è¯¯: {e}")
        return None
    finally:
        if driver: driver.quit()

def probe_correct_url(base_url, first_path):
    # å»æ‰æ–‡ä»¶åä¸­çš„å‚æ•°
    clean_name = first_path.split('?')[0].lstrip("/")
    
    print(f"ğŸ” æ­£åœ¨æ¢æµ‹è·¯å¾„ (æµ‹è¯•æ–‡ä»¶: {clean_name})...")
    
    base_urls = [base_url]
    if "bookh." in base_url:
        base_urls.append(base_url.replace("bookh.", "book."))

    patterns = [
        "{base}/files/large/{path}",
        "{base}/files/mobile/{path}",
        "{base}/{path}",
        "{base}/large/{path}",
        "{base}/mobile/{path}"
    ]

    for base in base_urls:
        for pattern in patterns:
            test_url = pattern.format(base=base, path=clean_name)
            try:
                r = requests.get(test_url, headers=get_headers(), timeout=5, stream=True, allow_redirects=True)
                if r.status_code == 200:
                    print(f"âœ… è·¯å¾„é€šäº†ï¼")
                    return pattern.format(base=base, path="{path}")
            except:
                pass
    return None

def download_image_task(args):
    url_template, filename, save_path, index = args
    try:
        clean_name = filename.split('?')[0].lstrip("/")
        final_url = url_template.format(path=clean_name)
        
        r = requests.get(final_url, headers=get_headers(), timeout=15)
        
        if r.status_code == 200:
            with open(save_path, "wb") as f: f.write(r.content)
            print(f"âœ… P{index} OK")
            return
        else:
            print(f"âŒ P{index} å¤±è´¥ ({r.status_code})")
    except Exception as e:
        print(f"âŒ P{index} é”™è¯¯: {e}")

# ã€ä¿®æ”¹ç‚¹2ã€‘æ–°å¢ PDF ç”Ÿæˆå‡½æ•°
def generate_pdf(folder_path, pdf_name):
    print("-" * 30)
    print(f"ğŸ“‘ æ­£åœ¨åˆæˆ PDF: {pdf_name}")
    
    images = []
    files = [f for f in os.listdir(folder_path) if f.endswith(('.jpg', '.webp', '.png'))]
    # æŒ‰æ–‡ä»¶åæ’åº (ç¡®ä¿ 001, 002 é¡ºåºæ­£ç¡®)
    files.sort()
    
    if not files:
        print("âŒ æ–‡ä»¶å¤¹ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆ PDF")
        return

    for f in files:
        try:
            full_path = os.path.join(folder_path, f)
            img = Image.open(full_path)
            # PDF ä¸æ”¯æŒ RGBA (é€æ˜é€šé“)ï¼Œå¦‚æœæ˜¯ WebP éœ€è¦è½¬ RGB
            if img.mode != "RGB":
                img = img.convert("RGB")
            images.append(img)
        except Exception as e:
            print(f"âš ï¸ è·³è¿‡æŸåå›¾ç‰‡: {f}")

    if images:
        try:
            output_path = os.path.join(folder_path, pdf_name)
            images[0].save(output_path, "PDF", resolution=100.0, save_all=True, append_images=images[1:])
            print(f"ğŸ‰ PDF ç”ŸæˆæˆåŠŸï¼æ–‡ä»¶ä½äº: {output_path}")
        except Exception as e:
            print(f"âŒ PDF ç”Ÿæˆå¤±è´¥: {e}")
    else:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„å›¾ç‰‡ç”¨äºåˆæˆ PDF")

def main():
    print("äº‘å±•ç½‘ä¸‹è½½å™¨ v11.0 (å…¨è‡ªåŠ¨éšèº« + PDF)")
    print("="*40)
    
    while True:
        url = input("è¯·è¾“å…¥é“¾æ¥ (q é€€å‡º):\n>>> ").strip()
        if url.lower() == 'q': break
        if not url: continue
        
        # 1. è§£å¯†
        data = fetch_book_data(url)
        if not data or not data.get("pages"):
            print("âŒ è§£å¯†å¤±è´¥ï¼Œæœªè·å–åˆ°é¡µé¢ã€‚")
            continue
            
        pages = data["pages"]
        title = data.get("title", "book")
        print(f"âœ… æå–åˆ° {len(pages)} é¡µã€‚")
        
        # 2. ç¡®å®šåŸºå‡† URL
        base_url = url.split("?")[0]
        if "/mobile/" in base_url: base_url = base_url.split("/mobile/")[0]
        elif "/files/" in base_url: base_url = base_url.split("/files/")[0]
        else: base_url = os.path.dirname(base_url)
        
        # 3. æ™ºèƒ½æ¢æµ‹
        url_template = ""
        if not pages[0].startswith("http"):
            url_template = probe_correct_url(base_url, pages[0])
            if not url_template:
                print("âŒ æ‰€æœ‰è·¯å¾„å°è¯•å‡å¤±è´¥ã€‚")
                continue
        else:
            url_template = "{path}" 
        
        # 4. å‡†å¤‡ä¸‹è½½
        safe_title = "".join([c for c in title if c.isalnum() or c in (' ', '_', '-')]).strip()
        folder = f"{safe_title}_{int(time.time())}"
        os.makedirs(folder, exist_ok=True)
        
        tasks = []
        for i, p in enumerate(pages):
            ext = "webp"
            if ".jpg" in p: ext = "jpg"
            if ".png" in p: ext = "png"
            
            save_path = os.path.join(folder, f"{i+1:03d}.{ext}")
            tasks.append((url_template, p, save_path, i+1))
            
        print(f"ğŸš€ å¯åŠ¨ä¸‹è½½...")
        with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
            executor.map(download_image_task, tasks)
        
        # 5. ç”Ÿæˆ PDF
        generate_pdf(folder, f"{safe_title}.pdf")
        
        print("ğŸ‰ å…¨éƒ¨æµç¨‹ç»“æŸï¼\n")

if __name__ == "__main__":
    main()